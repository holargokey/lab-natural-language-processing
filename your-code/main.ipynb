{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data = pd.read_csv(\"kg_train.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "data = data.head(1000)\n",
    "print(data.shape)\n",
    "data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>DEAR BELOVED,I am Sussan AdamsPLEASE ENDEAVOUR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Pls print.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>We are meeting with you at noon to discuss.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>Pls print.H &lt;hrod17@clintonemail.com&gt;Monday Fe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>Ok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>FYI Ã¢ÂÂ today the SFRC will take up at its ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>thanks</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>Dear friend,=20It is indeed my pleasure to wri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>Cherie Blair &lt;Tuesday September 29 2009 10:18 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>As you read this, I don't want you to feel sor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>Lisa ling.She's an oprah correspondant. we can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>I heard on the radio that there is a Cabinet m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>FYI</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>What does that mean for our schedules?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>&lt;P&gt;From The Desk Of&amp;nbsp; ALI SAITA&amp;nbsp; &lt;BR&gt;...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Quick summary of Burma elections:While there w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Pls print two copies.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>On secure</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>DeclanThanks for your email. Of course it's no...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>Dear Nancy--I very much want to meet and we've...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "397  DEAR BELOVED,I am Sussan AdamsPLEASE ENDEAVOUR...      1\n",
       "124                                         Pls print.      0\n",
       "219        We are meeting with you at noon to discuss.      0\n",
       "960  Pls print.H <hrod17@clintonemail.com>Monday Fe...      0\n",
       "837                                                 Ok      0\n",
       "220  FYI Ã¢ÂÂ today the SFRC will take up at its ...      0\n",
       "230                                             thanks      0\n",
       "976  Dear friend,=20It is indeed my pleasure to wri...      1\n",
       "885  Cherie Blair <Tuesday September 29 2009 10:18 ...      0\n",
       "781  As you read this, I don't want you to feel sor...      1\n",
       "736  Lisa ling.She's an oprah correspondant. we can...      0\n",
       "718  I heard on the radio that there is a Cabinet m...      0\n",
       "627                                                FYI      0\n",
       "270             What does that mean for our schedules?      0\n",
       "195  <P>From The Desk Of&nbsp; ALI SAITA&nbsp; <BR>...      1\n",
       "163  Quick summary of Burma elections:While there w...      0\n",
       "492                              Pls print two copies.      0\n",
       "636                                          On secure      0\n",
       "602  DeclanThanks for your email. Of course it's no...      0\n",
       "824  Dear Nancy--I very much want to meet and we've...      0"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text']\n",
    "y = data['label']\n",
    "#Dividing the training and test sets into two partitions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#loading the datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Punctuations: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "Stopwords: ['here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each']\n"
     ]
    }
   ],
   "source": [
    "print(f\" Punctuations: {string.punctuation}\") #provides a string containing all common punctuation marks\n",
    "print(f\"Stopwords: {stopwords.words('english')[100:110]}\")#fetches a list of common English stopwords(such as the, is)\n",
    "snowball = SnowballStemmer('english') #a stemming algorithm that reduces words to their root forms eg running to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "#Testing the stemmer\n",
    "word = 'running'\n",
    "print(snowball.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #module in Python is used for working with regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function data cleaning\n",
    "def text_cleaning(input):\n",
    "    #removing inline Javascript(e.g., <script>....)\n",
    "    input = re.sub(r'<script.*?>.*?</script>', '', input, flags=re.DOTALL)\n",
    "    #removing Html comments including the regular tags\n",
    "    input = re.sub(r'<.*?>', '', input)\n",
    "    #Remove all the special characters\n",
    "    input = re.sub(r'[^A-Za-z0-9 .,!?\\'\"-]', '', input)\n",
    "    #Remove numbers or digits \n",
    "    input = re.sub(r'\\d+', '', input)\n",
    "    #Remove all single characters\n",
    "    input = re.sub(r'\\b\\w\\b', '', input)\n",
    "    # Remove extra spaces created after removing single characters\n",
    "    input = re.sub(r'\\s+', ' ', input).strip()\n",
    "    #Remove single characters from the start\n",
    "    #input = re.sub(r'^\\s*\\w\\s*', '', input)\n",
    "    #Substitute multiple spaces with single space\n",
    "    input = re.sub(r'\\s+', ' ', input)\n",
    "    #Remove prefixed 'b'\n",
    "    input = re.sub(r'^b\\s*', '', input, flags=re.IGNORECASE)\n",
    "    #Convert to Lowercase\n",
    "    input = input.lower()\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cleaned = X_train.apply(text_cleaning)\n",
    "X_test_cleaned = X_test.apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "944    'll follow upmight make most sense for wjc to ...\n",
       "710    greetings you must be aware now that my countr...\n",
       "742    ..dear friend, am mr mark boland the bank mana...\n",
       "637    from dr sani mustapha,the manager of auditand ...\n",
       "378            release in partbbsee orig traffic bi mine\n",
       "933    dear friendci am mrevincent nicholasc am an ac...\n",
       "812    accept certified and notable bank cheques from...\n",
       "876                  already talked megan about changes.\n",
       "134                                                  fyi\n",
       "204    dear sir, am pleased to introduce business opp...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cleaned.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\somoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#download \n",
    "nltk.download(\"stopwords\")\n",
    "#define a function for stopwords removal\n",
    "def remove_stopwords(input):\n",
    "    #get the list of the common English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #split the text into words\n",
    "    words = input.split()\n",
    "    #filter out stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    #join the filtered words back into a string\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_without_stopwords = X_train_cleaned.apply(remove_stopwords)\n",
    "X_test_without_stopwords = X_test_cleaned.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521    dear sirc wish go offer consider partnerei mre...\n",
       "737    take mind balkans second see great plug global...\n",
       "740                             pls keep updates coming!\n",
       "660    christ bethel hospital rue abobote,abidjanivor...\n",
       "411    sbwhoeopfriday february amhre bravo! brava! is...\n",
       "                             ...                        \n",
       "408                sorry yes exactlywe shy tomorrow too.\n",
       "332    dearcgood dayei know message come suprise cons...\n",
       "208                                                  fyi\n",
       "613    greetings dear friend please permit contact me...\n",
       "78                    car way airport. talk? call berry.\n",
       "Name: text, Length: 200, dtype: object"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\somoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\somoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\somoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\somoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') #punkt tokenizer is a pre-trained model in NLTK used for splitting text into sentences and words.\n",
    "nltk.download('omw-1.4')  # WordNet's multilingual support\n",
    "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#create a function fo the Part of speech tag\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    #convert treebank POS tags to WordNet POS tag\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN #using Noun as default\n",
    "\n",
    "#define the lemmatization function, the POS tag helps to define a better lemmatization\n",
    "def lemmatize_with_pos(input_text):\n",
    "    words = word_tokenize(input_text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #Get POS tag for words\n",
    "    pos_tags = pos_tag(words)\n",
    "    #lemmatize with POS tags\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word.lower(), get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lemmatized = X_train_without_stopwords.apply(lemmatize_with_pos)\n",
    "X_test_lemmatized = X_test_without_stopwords.apply(lemmatize_with_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all this cleaning together\n",
    "\n",
    "def re_fresh(row):\n",
    "  return \" \".join(row)\n",
    "    \n",
    "X_train_fresh = X_train_lemmatized.apply(re_fresh)\n",
    "X_test_fresh = X_test_lemmatized.apply(re_fresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Cleaned</th>\n",
       "      <th>Without_Stopwords</th>\n",
       "      <th>Lemmatized</th>\n",
       "      <th>Fresh data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>Below is Palau's statement on the recent meeti...</td>\n",
       "      <td>elow is palau' statement on the recent meeting...</td>\n",
       "      <td>elow palau' statement recent meeting required ...</td>\n",
       "      <td>[elow, palau, ', statement, recent, meeting, r...</td>\n",
       "      <td>elow palau ' statement recent meeting require ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>FYI below on French statement on Haiti and U.S...</td>\n",
       "      <td>fyi below on french statement on haiti and .. ...</td>\n",
       "      <td>fyi french statement haiti .. supportparis jan...</td>\n",
       "      <td>[fyi, french, statement, haiti, .., supportpar...</td>\n",
       "      <td>fyi french statement haiti .. supportparis jan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>ONE HUNDRED AND FIFTY TWO MILLION DOLLARS...</td>\n",
       "      <td>one hundred and fifty two million dollars dear...</td>\n",
       "      <td>one hundred fifty two million dollars dear sir...</td>\n",
       "      <td>[one, hundred, fifty, two, million, dollar, de...</td>\n",
       "      <td>one hundred fifty two million dollar dear sirm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>1944 Ã¯Â¿Â½ February 14, 2005} married to Naze...</td>\n",
       "      <td>february , married to nazek audi hariri, was l...</td>\n",
       "      <td>february , married nazek audi hariri, lebanese...</td>\n",
       "      <td>[february, ,, married, nazek, audi, hariri, ,,...</td>\n",
       "      <td>february , married nazek audi hariri , lebanes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>From the Desk of =3A Dr biko zulato=5F=5F=5...</td>\n",
       "      <td>from the desk of dr biko zulatofffffffffffffff...</td>\n",
       "      <td>desk dr biko zulatoffffffffffffffffff fffffff ...</td>\n",
       "      <td>[desk, dr, biko, zulatoffffffffffffffffff, fff...</td>\n",
       "      <td>desk dr biko zulatoffffffffffffffffff fffffff ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>He released his hold. No need for the call.</td>\n",
       "      <td>he released his hold. no need for the call.</td>\n",
       "      <td>released hold. need call.</td>\n",
       "      <td>[release, hold, ., need, call, .]</td>\n",
       "      <td>release hold . need call .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>{ONE HUNDRED AND TWENTY SIX MILLION DOLLARS)De...</td>\n",
       "      <td>one hundred and twenty six million dollarsdear...</td>\n",
       "      <td>one hundred twenty six million dollarsdear sir...</td>\n",
       "      <td>[one, hundred, twenty, six, million, dollarsde...</td>\n",
       "      <td>one hundred twenty six million dollarsdear sir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>Great interview w/ her on cnn or msnbc where s...</td>\n",
       "      <td>great interview her on cnn or msnbc where she ...</td>\n",
       "      <td>great interview cnn msnbc talked impt strategi...</td>\n",
       "      <td>[great, interview, cnn, msnbc, talk, impt, str...</td>\n",
       "      <td>great interview cnn msnbc talk impt strategic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>The Director,            SEEKING FOR IMMEDIATE...</td>\n",
       "      <td>the director, seeking for immediate assistance...</td>\n",
       "      <td>director, seeking immediate assistance transfe...</td>\n",
       "      <td>[director, ,, seek, immediate, assistance, tra...</td>\n",
       "      <td>director , seek immediate assistance transfer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>Remind me to discuss when we talk</td>\n",
       "      <td>remind me to discuss when we talk</td>\n",
       "      <td>remind discuss talk</td>\n",
       "      <td>[remind, discus, talk]</td>\n",
       "      <td>remind discus talk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Original  \\\n",
       "556  Below is Palau's statement on the recent meeti...   \n",
       "472  FYI below on French statement on Haiti and U.S...   \n",
       "514       ONE HUNDRED AND FIFTY TWO MILLION DOLLARS...   \n",
       "807  1944 Ã¯Â¿Â½ February 14, 2005} married to Naze...   \n",
       "390     From the Desk of =3A Dr biko zulato=5F=5F=5...   \n",
       "82         He released his hold. No need for the call.   \n",
       "776  {ONE HUNDRED AND TWENTY SIX MILLION DOLLARS)De...   \n",
       "782  Great interview w/ her on cnn or msnbc where s...   \n",
       "628  The Director,            SEEKING FOR IMMEDIATE...   \n",
       "888                  Remind me to discuss when we talk   \n",
       "\n",
       "                                               Cleaned  \\\n",
       "556  elow is palau' statement on the recent meeting...   \n",
       "472  fyi below on french statement on haiti and .. ...   \n",
       "514  one hundred and fifty two million dollars dear...   \n",
       "807  february , married to nazek audi hariri, was l...   \n",
       "390  from the desk of dr biko zulatofffffffffffffff...   \n",
       "82         he released his hold. no need for the call.   \n",
       "776  one hundred and twenty six million dollarsdear...   \n",
       "782  great interview her on cnn or msnbc where she ...   \n",
       "628  the director, seeking for immediate assistance...   \n",
       "888                  remind me to discuss when we talk   \n",
       "\n",
       "                                     Without_Stopwords  \\\n",
       "556  elow palau' statement recent meeting required ...   \n",
       "472  fyi french statement haiti .. supportparis jan...   \n",
       "514  one hundred fifty two million dollars dear sir...   \n",
       "807  february , married nazek audi hariri, lebanese...   \n",
       "390  desk dr biko zulatoffffffffffffffffff fffffff ...   \n",
       "82                           released hold. need call.   \n",
       "776  one hundred twenty six million dollarsdear sir...   \n",
       "782  great interview cnn msnbc talked impt strategi...   \n",
       "628  director, seeking immediate assistance transfe...   \n",
       "888                                remind discuss talk   \n",
       "\n",
       "                                            Lemmatized  \\\n",
       "556  [elow, palau, ', statement, recent, meeting, r...   \n",
       "472  [fyi, french, statement, haiti, .., supportpar...   \n",
       "514  [one, hundred, fifty, two, million, dollar, de...   \n",
       "807  [february, ,, married, nazek, audi, hariri, ,,...   \n",
       "390  [desk, dr, biko, zulatoffffffffffffffffff, fff...   \n",
       "82                   [release, hold, ., need, call, .]   \n",
       "776  [one, hundred, twenty, six, million, dollarsde...   \n",
       "782  [great, interview, cnn, msnbc, talk, impt, str...   \n",
       "628  [director, ,, seek, immediate, assistance, tra...   \n",
       "888                             [remind, discus, talk]   \n",
       "\n",
       "                                            Fresh data  \n",
       "556  elow palau ' statement recent meeting require ...  \n",
       "472  fyi french statement haiti .. supportparis jan...  \n",
       "514  one hundred fifty two million dollar dear sirm...  \n",
       "807  february , married nazek audi hariri , lebanes...  \n",
       "390  desk dr biko zulatoffffffffffffffffff fffffff ...  \n",
       "82                          release hold . need call .  \n",
       "776  one hundred twenty six million dollarsdear sir...  \n",
       "782  great interview cnn msnbc talk impt strategic ...  \n",
       "628  director , seek immediate assistance transfer ...  \n",
       "888                                 remind discus talk  "
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Join X_train, X_tain_cleaned, X_train_without_stopwords and X_train_lemmatized into a dataframe table\n",
    "X_train_combined = pd.concat([X_train, X_train_cleaned, X_train_without_stopwords, X_train_lemmatized, X_train_fresh], axis = 1)\n",
    "X_test_combined = pd.concat([X_test, X_test_cleaned, X_test_without_stopwords, X_test_lemmatized, X_test_fresh], axis = 1)\n",
    "#rename the columns\n",
    "X_train_combined.columns = ['Original', 'Cleaned', 'Without_Stopwords', 'Lemmatized', 'Fresh data']\n",
    "X_test_combined.columns = ['Original', 'Cleaned', 'Without_Stopwords', 'Lemmatized', 'Fresh data']\n",
    "#view \n",
    "X_train_combined.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data BOW Matrix:\n",
      "[[0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [5 1 1 ... 1 4 2]\n",
      " [4 4 2 ... 0 2 2]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "##Create a bag of words using top 10 words\n",
    "\n",
    "##let's take only the most common 10 words\n",
    "bow_vect = CountVectorizer(max_features=10)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_data = bow_vect.fit_transform(X_train_fresh.tolist()).toarray()\n",
    "\n",
    "# Transform the test data\n",
    "X_test_data = bow_vect.transform(X_test_fresh.tolist()).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Fresh data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:175\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\index_class_helper.pxi:70\u001b[0m, in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Fresh data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[381], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m X_train_combined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuspicious_words\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_train_combined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFresh data\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(suspicious_words)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      7\u001b[0m X_train_combined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_len\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_train_combined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFresh data\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x)) \n\u001b[1;32m----> 9\u001b[0m X_test_combined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoney_mark\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_test_fresh[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFresh data\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(money_simbol_list)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     10\u001b[0m X_test_combined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuspicious_words\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_test_fresh[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFresh data\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(suspicious_words)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     11\u001b[0m X_test_combined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_len\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m X_test_fresh[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFresh data\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Fresh data'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"€\",\"$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "X_train_combined['money_mark'] = X_train_combined['Fresh data'].str.contains(money_simbol_list)*1\n",
    "X_train_combined['suspicious_words'] = X_train_combined['Fresh data'].str.contains(suspicious_words)*1\n",
    "X_train_combined['text_len'] = X_train_combined['Fresh data'].apply(lambda x: len(x)) \n",
    "\n",
    "X_test_combined['money_mark'] = X_test_fresh['Fresh data'].str.contains(money_simbol_list)*1\n",
    "X_test_combined['suspicious_words'] = X_test_fresh['Fresh data'].str.contains(suspicious_words)*1\n",
    "X_test_combined['text_len'] = X_test_fresh['Fresh data'].apply(lambda x: len(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would you create a Bag of Words with the CountVectorizer method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task (optional) - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "Use a MultinimialNB with default parameters.\n",
    "\n",
    "Your task is to find the **best feature representation**.\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2,random_state=100)\n",
    "kmeans.fit(X_train_data)\n",
    "pred = kmeans.predict(X_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521    0\n",
       "737    1\n",
       "740    1\n",
       "660   -1\n",
       "411    1\n",
       "      ..\n",
       "408    1\n",
       "332    0\n",
       "208    1\n",
       "613    0\n",
       "78     1\n",
       "Name: label, Length: 200, dtype: int64"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred-y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
